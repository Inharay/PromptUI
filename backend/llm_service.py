# from openai import OpenAI
# import os
# import time

# class LLMService:
#     def __init__(self):
#         # 配置 OpenAI 客户端以连接到开源大模型
#         # 这里默认配置为连接本地运行的 Ollama 服务 (http://localhost:11434/v1)
#         # 如果您使用其他服务（如 vLLM, LM Studio 等），请修改 base_url 和 api_key
#         self.client = OpenAI(
#             base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
#             api_key="sk-c13d96e9c2b0486bb3a2c2ed6016e9b5"  # 本地服务通常不需要真实的 API Key
#         )
#         # 请确保您的模型名称与本地运行的模型一致，例如 "llama3", "qwen2", "mistral" 等
#         self.model = "qwen3-max" 

#     def chat(self, message: str, mode: str) -> str:
#         """
#         Process the chat message and return a response from the LLM.
#         mode: 'smart-query' or 'unstructured'
#         """
#         # Normalize mode
#         if mode == "smart":
#             mode = "smart-query"

#         system_prompt = "你是一个智能助手。"
#         if mode == "smart-query":
#             system_prompt = "你是一个数据分析专家。请根据用户的问题，模拟查询数据库并给出专业的数据分析回答。"
#         else:
#             system_prompt = "你是一个非结构化文本分析专家。请对用户输入的内容进行深度解析和提炼。"

#         try:
#             response = self.client.chat.completions.create(
#                 model=self.model,
#                 messages=[
#                     {"role": "system", "content": system_prompt},
#                     {"role": "user", "content": message}
#                 ],
#                 temperature=0.7,
#             )
#             return response.choices[0].message.content
#         except Exception as e:
#             print(f"Error calling LLM: {e}")
#             # 模拟数据返回，以便在没有 LLM 的情况下也能展示表格
#             return f"调用大模型失败: {str(e)}。请确保您已启动本地大模型服务（如 Ollama），并检查模型名称配置。"

#     def analyze_file(self, file_path: str, mode: str) -> dict:
#         """
#         Analyze an uploaded file and return result with a generated file.
#         """
#         filename = os.path.basename(file_path)
        
#         # 尝试读取文件内容（假设是文本文件）
#         file_content = ""
#         try:
#             with open(file_path, "r", encoding="utf-8") as f:
#                 file_content = f.read(10000) # 读取前10000个字符作为上下文
#         except Exception:
#             file_content = "(文件内容无法读取或非文本文件)"

#         prompt = f"请分析以下文件内容（文件名：{filename}）：\n\n{file_content}\n\n请生成一份详细的分析报告。"
        
#         try:
#             analysis_result = self.chat(prompt, mode)
#         except Exception as e:
#             analysis_result = f"分析失败: {str(e)}"

#         # Generate a result file
#         output_dir = "outputs"
#         os.makedirs(output_dir, exist_ok=True)
        
#         result_filename = f"analysis_report_{filename}.txt"
#         result_path = os.path.join(output_dir, result_filename)
        
#         with open(result_path, "w", encoding="utf-8") as f:
#             f.write(f"Analysis Report for {filename}\n")
#             f.write("="*30 + "\n")
#             f.write(f"Mode: {mode}\n")
#             f.write(f"Generated by: {self.model}\n")
#             f.write("-" * 20 + "\n\n")
#             f.write(analysis_result)
            
#         return {
#             "message": f"文件 {filename} 分析完成，请查看生成的报告。",
#             "generated_file": {
#                 "name": result_filename,
#                 "path": result_path,
#                 "size": os.path.getsize(result_path)
#             }
#         }

#     def extract_data_from_file(self, file_path: str) -> dict:
#         """
#         专门用于非结构化提取的方法。
#         读取上传文件，调用大模型提取信息，并返回结果文件。
#         """
#         filename = os.path.basename(file_path)
        
#         # 读取文件内容
#         file_content = ""
#         try:
#             with open(file_path, "r", encoding="utf-8") as f:
#                 file_content = f.read(10000)
#         except Exception:
#             file_content = "(文件内容无法读取或非文本文件)"

#         # 构造提取专用的 Prompt
#         prompt = f"请对以下文件内容（文件名：{filename}）进行非结构化信息提取。请将关键信息提取为结构化的格式（如JSON或Markdown表格），并忽略无关信息。\n\n内容：\n{file_content}"
        
#         try:
#             # 复用 chat 方法调用大模型，mode 传入 extraction 以便区分（虽然 chat 方法目前对 extraction 没特殊处理，但可以扩展）
#             extraction_result = self.chat(prompt, "unstructured-extraction")
#         except Exception as e:
#             extraction_result = f"提取失败: {str(e)}"

#         # 生成提取结果文件
#         output_dir = "outputs"
#         os.makedirs(output_dir, exist_ok=True)
        
#         result_filename = f"extraction_result_{filename}.txt"
#         result_path = os.path.join(output_dir, result_filename)
        
#         with open(result_path, "w", encoding="utf-8") as f:
#             f.write(f"Extraction Result for {filename}\n")
#             f.write("="*30 + "\n")
#             f.write(f"Generated by: {self.model}\n")
#             f.write("-" * 20 + "\n\n")
#             f.write(extraction_result)
            
#         return {
#             "message": f"文件 {filename} 提取完成，请下载结果文件。",
#             "generated_file": {
#                 "name": result_filename,
#                 "path": result_path,
#                 "size": os.path.getsize(result_path)
#             }
#         }
