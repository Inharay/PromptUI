from openai import OpenAI
import os
import time

class LLMService:
    def __init__(self):
        # 配置 OpenAI 客户端以连接到开源大模型
        # 这里默认配置为连接本地运行的 Ollama 服务 (http://localhost:11434/v1)
        # 如果您使用其他服务（如 vLLM, LM Studio 等），请修改 base_url 和 api_key
        self.client = OpenAI(
            base_url="http://localhost:11434/v1",
            api_key="ollama"  # 本地服务通常不需要真实的 API Key
        )
        # 请确保您的模型名称与本地运行的模型一致，例如 "llama3", "qwen2", "mistral" 等
        self.model = "qwen2.5:7b" 

    def chat(self, message: str, mode: str) -> str:
        """
        Process the chat message and return a response from the LLM.
        mode: 'smart-query' or 'unstructured'
        """
        # Normalize mode
        if mode == "smart":
            mode = "smart-query"

        system_prompt = "你是一个智能助手。"
        if mode == "smart-query":
            system_prompt = "你是一个数据分析专家。请根据用户的问题，模拟查询数据库并给出专业的数据分析回答。"
        else:
            system_prompt = "你是一个非结构化文本分析专家。请对用户输入的内容进行深度解析和提炼。"

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": message}
                ],
                temperature=0.7,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error calling LLM: {e}")
            # 模拟数据返回，以便在没有 LLM 的情况下也能展示表格
            if mode == "smart-query":
                return """
调用大模型失败，已切换到模拟数据模式。

根据您的查询，以下是本季度的销售数据报表：

| 部门 | 第一季度 | 第二季度 | 环比增长 |
| :--- | :---: | :---: | :---: |
| 研发部 | 120万 | 150万 | +25% |
| 市场部 | 80万 | 95万 | +18.7% |
| 销售部 | 200万 | 280万 | +40% |
| **总计** | **400万** | **525万** | **+31.2%** |

**分析建议**：
1. 销售部增长最为显著，建议继续加大投入。
2. 市场部表现稳健，可尝试新的营销渠道。
"""
            else:
                return f"调用大模型失败: {str(e)}。请确保您已启动本地大模型服务（如 Ollama），并检查模型名称配置。"

    def analyze_file(self, filename: str, mode: str) -> dict:
        """
        Analyze an uploaded file and return result with a generated file.
        """
        # 尝试读取文件内容（假设是文本文件）
        file_path = os.path.join("uploads", filename)
        file_content = ""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                file_content = f.read(10000) # 读取前10000个字符作为上下文
        except Exception:
            file_content = "(文件内容无法读取或非文本文件)"

        prompt = f"请分析以下文件内容（文件名：{filename}）：\n\n{file_content}\n\n请生成一份详细的分析报告。"
        
        try:
            analysis_result = self.chat(prompt, mode)
        except Exception as e:
            analysis_result = f"分析失败: {str(e)}"

        # Generate a result file
        output_dir = "outputs"
        os.makedirs(output_dir, exist_ok=True)
        
        result_filename = f"analysis_report_{filename}.txt"
        result_path = os.path.join(output_dir, result_filename)
        
        with open(result_path, "w", encoding="utf-8") as f:
            f.write(f"Analysis Report for {filename}\n")
            f.write("="*30 + "\n")
            f.write(f"Mode: {mode}\n")
            f.write(f"Generated by: {self.model}\n")
            f.write("-" * 20 + "\n\n")
            f.write(analysis_result)
            
        return {
            "message": f"文件 {filename} 分析完成，请查看生成的报告。",
            "generated_file": {
                "name": result_filename,
                "path": result_path,
                "size": os.path.getsize(result_path)
            }
        }
