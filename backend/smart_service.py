from openai import OpenAI
import os
import time
import json
from .config import SMART_OUTPUT_DIR

class SmartService:
    def __init__(self):
        # 配置 OpenAI 客户端
        self.client = OpenAI(
            base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
            api_key="sk-c13d96e9c2b0486bb3a2c2ed6016e9b5"
        )
        self.model = "qwen3-max" 
        self.history = {} # In-memory storage for chat history: {conversation_id: [messages]}

    def chat_stream(self, prompt: str, conversation_id: str, employee_id: str):
        """
        Smart Analysis Chat with History (Streaming)
        """
        system_prompt = "你是一个数据分析专家。请根据用户的问题，模拟查询数据库并给出专业的数据分析回答。"
        
        # Initialize history for this conversation if not exists
        if conversation_id not in self.history:
            self.history[conversation_id] = []

        # Add user message to history
        self.history[conversation_id].append({"role": "user", "content": prompt})

        # Construct messages for the API call
        messages = [{"role": "system", "content": system_prompt}] + self.history[conversation_id]

        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                stream=True
            )
            
            full_response = ""
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content
            
            # Add assistant response to history
            self.history[conversation_id].append({"role": "assistant", "content": full_response})
            
            # Simulate Citations
            citations = [
                {"title": "2023年财务报表.xlsx", "url": "#", "source": "结构化知识库"},
                {"title": "Q4销售数据汇总.csv", "url": "#", "source": "结构化知识库"}
            ]
            yield "\n\n__CITATIONS__\n\n"
            yield json.dumps(citations)

        except Exception as e:
            print(f"Error calling LLM: {e}")
            yield f"调用大模型失败: {str(e)}。"

    def chat(self, prompt: str, conversation_id: str, employee_id: str) -> str:
        """
        Smart Analysis Chat with History
        """
        system_prompt = "你是一个数据分析专家。请根据用户的问题，模拟查询数据库并给出专业的数据分析回答。"
        
        # Initialize history for this conversation if not exists
        if conversation_id not in self.history:
            self.history[conversation_id] = []

        # Add user message to history
        self.history[conversation_id].append({"role": "user", "content": prompt})

        # Construct messages for the API call
        messages = [{"role": "system", "content": system_prompt}] + self.history[conversation_id]

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
            )
            response_content = response.choices[0].message.content
            
            # Add assistant response to history
            self.history[conversation_id].append({"role": "assistant", "content": response_content})
            
            return response_content
        except Exception as e:
            print(f"Error calling LLM: {e}")
            return f"调用大模型失败: {str(e)}。"

    def clear_history(self, conversation_id: str):
        """
        Clear chat history for a specific conversation
        """
        if conversation_id in self.history:
            del self.history[conversation_id]
            return True
        return False

    def get_history(self, conversation_id: str) -> list:
        """
        Get chat history for a specific conversation
        """
        return self.history.get(conversation_id, [])

    def analyze_file(self, file_path: str) -> dict:
        """
        智能问数文件分析
        """
        filename = os.path.basename(file_path)
        file_content = ""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                file_content = f.read(10000)
        except Exception:
            file_content = "(文件内容无法读取或非文本文件)"

        prompt = f"请分析以下文件内容（文件名：{filename}）：\n\n{file_content}\n\n请生成一份详细的分析报告。"
        
        try:
            # analysis_result = self.chat(prompt)
            analysis_result = "test"

        except Exception as e:
            analysis_result = f"分析失败: {str(e)}"

        result_filename = f"analysis_report_{filename}.txt"
        result_path = os.path.join(SMART_OUTPUT_DIR, result_filename)
        
        with open(result_path, "w", encoding="utf-8") as f:
            f.write(f"Analysis Report for {filename}\n")
            f.write("="*30 + "\n")
            f.write(f"Mode: Smart Query\n")
            f.write(f"Generated by: {self.model}\n")
            f.write("-" * 20 + "\n\n")
            f.write(analysis_result)
            
        return {
            "message": f"文件 {filename} 分析完成，请查看生成的报告。",
            "generated_file": {
                "name": result_filename,
                "path": result_path,
                "size": os.path.getsize(result_path)
            }
        }
